import { Log } from '../helpers/classes/log';
import {
    ResultBatch,
    ScrapeResult,
} from '../helpers/classes/result';

/**
 * Superclass for all "scrapers"
 *
 * @remarks
 * This abstract class describes a standardized method of scraping web pages and saving the
 * results. Its structure is specifically engineered to support complex, relational data stored
 * in a RDBMS such as Postgres. An subclass of AbstractScraper generally describes the process of
 * scraping one type of webpage into one database table. Each instance of a class extending
 * AbstractScraper corresponds to the scrape of one specific URL. The general use pattern for an
 * instance of such as class is to first call the constructor, then [[Scraper.scrape]].
 */
export abstract class Scraper {
    /**
     * Scrapers always check for a local copy of the target resource (using
     * [[Scraper.checkForLocalRecord]]) before executing a scrape from an external resource. If the
     * resource was found (and therefore no external calls made), this is set to true.
     */
    public dataReadFromLocal: boolean;

    /**
     * Contains all results generated by [[Scraper.scrape]], including recursive calls.
     */
    public results: ResultBatch;

    /**
     * A simple, human-readble description of *what* is being scraped. Used for logging.
     */
    public description: string;

    /**
     * Used to override .env settings and force-log the output of a given scraper.
     */
    public verbose: boolean;

    /**
     * Flag indicating a sucessful scrape, set to true after non-error-throwing call to
     * [[Scraper.scrape]].
     */
    public scrapeSucceeded: boolean;

    /**
     * @param url see [[Scraper.url]]
     * @param description see [[Scraper.description]]
     * @param verbose see [[Scraper.verbose]]
     */
    public constructor(
        description: string,
        verbose?: boolean,
    ) {
        this.verbose = verbose || false;
        this.description = description;
        this.results = new ResultBatch();
        this.dataReadFromLocal = false;
        this.scrapeSucceeded = false;
    }

    /**
     * Gets the local stored record corresponding to a given scraper. Should return null if no
     * local record is found. By default, returns false (resource is always scraped).
     */
    public checkForLocalRecord(): Promise<boolean> {
        return Promise.resolve(false);
    }

    /**
     * Extracts information from a scraped resource **synchronously**
     *
     * @remarks
     * Must be called after [[Scraper.requestScrape]].
     *
     * Extracted info should be stored into class properties, to be
     * saved later by [[Scraper.saveToLocal]]. Stores constructed (**but not .scrape()'ed**)
     * instances of any more recursive scrapes extracted from this one, to later be scraped by
     * [[Scraper.scrapeDependencies]]. By default, this method does nothing.
     */
    protected extractInfo(): void {
        Log.notify(`extractInfo() not implemented for ${this.description}`);
    }

    /**
     * Prints a detailed report of local properties for a scraper, used for debugging
     */
    public printInfo(): void {
        Log.notify('print info not implemented for this scraper');
    }

    /**
     * Simple CLI reporting tool for debugging unsuccessful scrapes
     */
    public printResult(): void {
        if(this.scrapeSucceeded === false) {
            Log.err(`Scrape failed for album url:\n${this.description}`);
        } else if(this.dataReadFromLocal) {
            Log.success(
                `Scrape unnecessary, record exists in database: ${this.description}`,
            );
        } else {
            Log.success(
                `Scrape successful\nURL: ${this.description}`,
            );
        }
    }

    /**
     * Requests and stores an external resource, to be parsed later by
     * [[Scraper.extractInfo]]. By default, nothing is requested.
     */
    public requestScrape(): Promise<void> {
        Log.notify(`requestScrape() not implemented for ${this.description}`);
        return Promise.resolve();
    }

    /**
     * Saves scraped, extracted, and parsed information into a local record. By default, does
     * nothing.
     *
     * @remarks
     * This method must be called after [[Scraper.requestScrape]],
     * [[Scraper.extractInfo]], and [[Scraper.scrapeDependencies]]
     *
     * @returns the entity that was saved
     */
    protected async saveToLocal(): Promise<void> {
        Log.notify(`saveToLocal() not implemented for ${this.description}`);
    }

    /**
     * Entry point for initiating an asset scrape. General scrape outline/method order:
     *
     * 1. [[Scraper.checkForLocalRecord]]
     * 2. If local entity was found, update class props and return.
     * 3. [[Scraper.requestScrape]]
     * 4. [[Scraper.extractInfo]]
     * 5. [[Scraper.scrapeDependencies]]
     * 6. [[Scraper.saveToLocal]]
     * 7. Update class props and return
     *
     * @remarks
     * This method should be considered *unsafe* - there are several points where this can throw
     * errors. This is intentional, and allows easier support for relational data scraping/storage.
     * Scraped assets may have a mixture of required and non-required dependencies, the status of
     * which should be kept in mind when implementing [[Scraper.scrapeDependencies]]. A subclass
     * should catch and log errors from non-required scrapes. However, errors from a required
     * scrape should remain uncaught, so the original call to a [[Scraper.scrape]] will error out
     * before [[Scraper.save]] is called for incomplete data.
     *
     * @param forceScrape If set to true, scrapes the external resource regardless of any existing
     * local records
     */
    public async scrape(forceScrape = false): Promise<void> {
        Log.notify(`Beginning Scrape of ${this.description}`);
        const recordExists = await this.checkForLocalRecord();
        if(recordExists && !forceScrape) {
            this.dataReadFromLocal = true;
            this.results.push(new ScrapeResult(true, this.description));
            this.scrapeSucceeded = true;
            Log.success(`Local Record Found for ${this.description}`);
            return;
        }
        await this.requestScrape();
        this.extractInfo();
        await this.scrapeDependencies();
        await this.saveToLocal();
        this.results.push(new ScrapeResult(true, this.description));
        this.scrapeSucceeded = true;
        Log.success(`Finished Scrape of ${this.description}`);
    }

    /**
     * Executes [[Scraper.scrape]] on any recursive scrapes found in the initial scrape.
     * Defaults to simply resolving an empty promise, so subclasses with no dependencies don't have
     * to implement this function. See [[Scraper.scrape]] for more information on implementation.
     *
     * @remarks
     * This method must be called after [[Scraper.requestScrape]] and
     * [[Scraper.extractInfo]]
     *
     * @returns the entity that was saved
     */
    protected scrapeDependencies(): Promise<void> {
        return Promise.resolve();
    }

    /**
     * Scrape the genres associated with this artist
     */
    public static async scrapeDependencyArr<T extends Scraper>(
        scrapers: T[],
        forceScrape = false,
    ): Promise<ScrapersWithResults<T>> {
        const dependencies: ScrapersWithResults<T> = {
            scrapers: [],
            results: new ResultBatch(),
        };
        if(scrapers != null && scrapers.length > 0) {
            for await(const scraper of scrapers) {
                try {
                    await scraper.scrape(forceScrape);
                    dependencies.scrapers.push(scraper);
                    dependencies.results.concat(scraper.results);
                } catch(err) {
                    Log.err(`ERROR SCRAPING ${scraper.description}: ${err.message}`);
                    dependencies.results.push(new ScrapeResult(
                        false,
                        scraper.description,
                        err,
                    ));
                }
            }
        }
        return dependencies;
    }
}

export interface ScrapersWithResults<T extends Scraper> {
    results: ResultBatch;
    scrapers: T[];
}
